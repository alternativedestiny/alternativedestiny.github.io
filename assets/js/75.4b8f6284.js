(window.webpackJsonp=window.webpackJsonp||[]).push([[75],{450:function(n,_,v){"use strict";v.r(_);var a=v(3),t=Object(a.a)({},(function(){var n=this,_=n._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[_("h1",{attrs:{id:"《统计学习方法-第-2-版-》-李航"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#《统计学习方法-第-2-版-》-李航"}},[n._v("#")]),n._v(" 《统计学习方法（第 2 版）》 李航")]),n._v(" "),_("h2",{attrs:{id:"_1-统计学习方法概论"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_1-统计学习方法概论"}},[n._v("#")]),n._v(" 1. 统计学习方法概论")]),n._v(" "),_("ol",[_("li",[n._v("统计学习")]),n._v(" "),_("li",[n._v("监督学习\n"),_("ol",[_("li",[n._v("基本概念")]),n._v(" "),_("li",[n._v("问题的形式化")])])]),n._v(" "),_("li",[n._v("统计学习三要素\n"),_("ol",[_("li",[n._v("模型")]),n._v(" "),_("li",[n._v("策略")]),n._v(" "),_("li",[n._v("算法")])])]),n._v(" "),_("li",[n._v("模型评估与模型选择\n"),_("ol",[_("li",[n._v("训练误差与测试误差")]),n._v(" "),_("li",[n._v("过拟合与模型选择")])])]),n._v(" "),_("li",[n._v("正则化与交叉验证\n"),_("ol",[_("li",[n._v("正则化")]),n._v(" "),_("li",[n._v("交叉验证")])])]),n._v(" "),_("li",[n._v("泛化能力\n"),_("ol",[_("li",[n._v("泛化误差")]),n._v(" "),_("li",[n._v("泛化误差上界")])])]),n._v(" "),_("li",[n._v("生成模型与判别模型")]),n._v(" "),_("li",[n._v("分类问题")]),n._v(" "),_("li",[n._v("标注问题")]),n._v(" "),_("li",[n._v("回归问题")])]),n._v(" "),_("h2",{attrs:{id:"_2-感知机"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_2-感知机"}},[n._v("#")]),n._v(" 2. 感知机")]),n._v(" "),_("ol",[_("li",[n._v("感知机模型")]),n._v(" "),_("li",[n._v("感知机学习策略\n"),_("ol",[_("li",[n._v("数据集的线性可分性")]),n._v(" "),_("li",[n._v("感知机学习策略")])])]),n._v(" "),_("li",[n._v("感知机学习算法\n"),_("ol",[_("li",[n._v("感知机学习算法的原始形式")]),n._v(" "),_("li",[n._v("算法的收敛性")]),n._v(" "),_("li",[n._v("感知机学习算法的对偶形式")])])])]),n._v(" "),_("h2",{attrs:{id:"_3-众近邻法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_3-众近邻法"}},[n._v("#")]),n._v(" 3. 众近邻法")]),n._v(" "),_("p",[n._v("3.1 k 近邻算法\n3.2 k 近邻模型\n3.2.1 模型\n3.2.2 距离度量\n·3.2.3 k 值的选择\n3.2.4 分类决策规则\n3.3k 近邻法的实现：kd 树\n3.3.1 构造 af 树\n3.3.2 搜索 af 树")]),n._v(" "),_("h2",{attrs:{id:"_4-朴素贝叶斯法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_4-朴素贝叶斯法"}},[n._v("#")]),n._v(" 4. 朴素贝叶斯法")]),n._v(" "),_("p",[n._v("4.1 朴素贝叶斯法的学习与分类\n4.1.1 基本方法\n4.1.2 后验概率最大化的含义\n4.2 朴素贝叶斯法的参数估计\n4.2.1 极大似然估计\n4.2.2 学习与分类算法\n4.2.3 贝叶斯估计")]),n._v(" "),_("h2",{attrs:{id:"_5-决策树"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_5-决策树"}},[n._v("#")]),n._v(" 5. 决策树")]),n._v(" "),_("p",[n._v("5.1 决策树模型与学习\n5.1.1 决策树模型\n5.1.2 决策树与 isthen 规则\n5.1.3 决策树与条件概率分布\n5.1.4 决策树学习\n5.2 特征选择\n5.2.1 特征选择问题\n5.2.2 信息增益\n5.2.3 信息增益比\n5.3 决策树的生成\n5.3.11d3 算法\n5.3.2 c4.5 的生成算法\n5.4 决策树的剪枝\n5.5cart 算法\n5.5.1cart 生成\n5.5.2cart 剪枝")]),n._v(" "),_("h2",{attrs:{id:"_6-逻辑斯谛回归与最大熵模型"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_6-逻辑斯谛回归与最大熵模型"}},[n._v("#")]),n._v(" 6. 逻辑斯谛回归与最大熵模型")]),n._v(" "),_("p",[n._v("6.1 逻辑斯谛回归模型\n6.1.1 逻辑斯谛分布\n6.1.2 项逻辑斯谛回归模型\n6.1.3 模型参数估计\n6.1.4 多项逻辑斯谛回归\n6.2 最大熵模型\n6.2.1 最大熵原理\n6.2.2 最大熵模型的定义\n6.2.3 最大熵模型的学习\n6.2.4 极大似然估计\n6.3 模型学习的最优化算法\n6.3.1 改进的迭代尺度法\n6.3.2 拟牛顿法")]),n._v(" "),_("h2",{attrs:{id:"_7-支持向量机"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_7-支持向量机"}},[n._v("#")]),n._v(" 7. 支持向量机")]),n._v(" "),_("p",[n._v("7.1 线性可分支持向量机与硬间隔最大化\n7.1.1 线性可分支持向量机\n7.1.2 函数间隔和几何间隔\n7.1.3 间隔最大化\n7.1.4 学习的对偶算法\n7.2 线性支持向量机与软间隔最大化\n7.2.1 线性支持向量机\n7.2.2 学习的对偶算法\n7.2.3 支持向量\n7.2.4 合页损失函数\n7.3 非线性支持向量机与核函数\n7.3.1 核技巧\n7.3.2 定核\n7.3.3 常用核函数\n7.3.4 非线性支持向量分类机\n7.4 序列最小最优化算法\n7.4.1 两个变量二次规划的求解方法\n7.4.2 变量的选择方法\n7.4.3smo 算法")]),n._v(" "),_("h2",{attrs:{id:"_8-提升方法"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_8-提升方法"}},[n._v("#")]),n._v(" 8. 提升方法")]),n._v(" "),_("p",[n._v("8.1 提升方法 adaboost 算法\n8.1.1 提升方法的基本思路\n8.1.2adaboost 算法\n8.1.3 adaboost 的例子\n8.2adaboost 算法的训练误差分析\n8.3 adaboost 算法的解释\n8.3.1 前向分步算法\n8.3.2 前向分步算法与 ad9boost\n8.4 提升树\n8.4.1 提升树模型\n8.4.2 提升树算法\n8.4.3 梯度提升")]),n._v(" "),_("h2",{attrs:{id:"_9-em-算法及其推广"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_9-em-算法及其推广"}},[n._v("#")]),n._v(" 9. em 算法及其推广")]),n._v(" "),_("p",[n._v("9.1em 算法的引入\n9.1.1em 算法\n9.1.2em 算法的导出\n9.1.3em 算法在非监督学习中的应用\n9.2em 算法的收敛性\n9.3em 算法在高斯混合模型学习中的应用\n9.3.1 高斯混合模型\n9.3.2 高斯混合模型参数估计的 em 算法\n9.4em 算法的推广\n9.4.1f 函数的极大极大算法\n9.4.2gem 算法")]),n._v(" "),_("h2",{attrs:{id:"_10-隐马尔可夫模型"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_10-隐马尔可夫模型"}},[n._v("#")]),n._v(" 10. 隐马尔可夫模型")]),n._v(" "),_("p",[n._v("10.1 隐马尔可夫模型的基本概念\n10.1.1 隐马尔可夫模型的定义\n10.1.2 观测序列的生成过程\n10.1.3 隐马尔可夫模型的 3 个基本问题\n10.2 概率计算算法\n10.2.1 直接计算法\n10.2.2 前向算法\n10.2.3 后向算法\n10.2.4 一些概率与期望值的计算\n10.3 学习算法\n10.3.1 监督学习方法\n10.3.2baum-welch 算法\n10.3.3baum-welch 模型参数估计公式\n10.4 预测算法\n10.4.1 近似算法\n10.4.2 维特比算法")]),n._v(" "),_("h2",{attrs:{id:"_11-条件随机场"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#_11-条件随机场"}},[n._v("#")]),n._v(" 11. 条件随机场")]),n._v(" "),_("ol",[_("li",[n._v("概率无向图模型\n"),_("ol",[_("li",[n._v("模型定义")]),n._v(" "),_("li",[n._v("概率无向图模型的因子分解")])])]),n._v(" "),_("li",[n._v("条件随机场的定义与形式\n"),_("ol",[_("li",[n._v("条件随机场的定义")]),n._v(" "),_("li",[n._v("条件随机场的参数化形式")]),n._v(" "),_("li",[n._v("条件随机场的简化形式")]),n._v(" "),_("li",[n._v("条件随机场的矩阵形式\n11.3 条件随机场的概率计算问题\n11.3.1 前向后向算法\n11.3.2 概率计算\n11.3.3 期望值的计算\n11.4 条件随机场的学习算法\n11.4.1 改进的迭代尺度法\n11.4.2 拟牛顿法\n11.5 条件随机场的预测算法")])])])])])}),[],!1,null,null,null);_.default=t.exports}}]);